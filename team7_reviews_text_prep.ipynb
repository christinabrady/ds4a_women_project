{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65971aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/christinabrady/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### run once to install nltk and download stopwords\n",
    "# !pip3 install nltk\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "769acc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams, bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f7ab697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169762, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_json(\"processed_data/yelp_team7_dataset_review.json\")\n",
    "pandemic_reviews = reviews[reviews.date >= datetime.datetime(2020, 2, 1)] \n",
    "pandemic_reviews.shape\n",
    "# stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cdc82c",
   "metadata": {},
   "source": [
    "### Number of Characters per Review (includes punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd16907",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_chars = [len(list(x)) for x in reviews.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66c28ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_chars_fig =px.histogram(review_chars,\n",
    "                          title = \"Number of characters in reviews\",\n",
    "                         labels = {'value': 'Number of characters'})\n",
    "review_chars_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2201360",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[\"text_char_len\"] = review_chars\n",
    "review_len_x_star_fig = px.box(reviews, \n",
    "                                  x = \"stars\", \n",
    "                                  y = \"text_char_len\", \n",
    "                                  title = \"Length of Review by Stars\")\n",
    "review_len_x_star_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797421a1",
   "metadata": {},
   "source": [
    "### Number of words per review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a68c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pull review text into its own list to make the code easier to read\n",
    "review_text = reviews.text.tolist()\n",
    "## before taking out ALL punctuation, let's take out apostrophes and leave the contraction combined\n",
    "review_text = [re.sub(\"'\", \"\", review).strip() for review in review_text]\n",
    "review_text_no_punc = [re.sub(\"[^a-zA-Z]\", \" \", review).strip() for review in review_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ebb7622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This review is more about the ambiance and drinks as thats all I had the chance to experience  Im in this area all the time and never noticed this place bc its not open until the afternoon evening I believe    Met a friend here for drinks and I love the laid back feel  industrial lighting and sleek interior  There werent too many people there so it was pretty quiet  The wait staff was a little slow initially taking my drink order while I waited on my party  but once she finally came over she was very attentive the rest of the evening    I had a glass of Chardonnay and my friend had a cocktail  She also ordered the Brussel sprouts which I tasted and they were delicious  I consider myself a sprout connoisseur so I give them a   out of       I will be back for sure to have the pizza and taste the wings'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's see if that worked\n",
    "review_text_no_punc[np.random.randint(0, len(review_text_no_punc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd8adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_review = [len(text.split()) for text in review_text_no_punc]\n",
    "words_per_review_fig =px.histogram(words_per_review,\n",
    "                          title = \"Number of words in reviews\",\n",
    "                         labels = {'value': 'Number of characters'})\n",
    "words_per_review_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc7c63dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bx/jfyslg2155v2dp86082zjd280000gn/T/ipykernel_36718/3293414076.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### What are the most common words overall?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview_text_no_punc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/bx/jfyslg2155v2dp86082zjd280000gn/T/ipykernel_36718/3293414076.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### What are the most common words overall?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview_text_no_punc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/codebase/ds4a_women_project/project_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m    129\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     return [\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     ]\n",
      "\u001b[0;32m~/Documents/codebase/ds4a_women_project/project_env/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     return [\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     ]\n",
      "\u001b[0;32m~/Documents/codebase/ds4a_women_project/project_env/lib/python3.8/site-packages/nltk/tokenize/destructive.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\" \\1 \\2 \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\" \\1 \\2 \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### What are the most common words overall?\n",
    "# tokens = [word_tokenize(words) for words in review_text_no_punc]\n",
    "tokens = [word.split() for words in review_text_no_punc for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c9b7de",
   "metadata": {},
   "source": [
    "We want to keep negatives as they probably have meaning for us."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
